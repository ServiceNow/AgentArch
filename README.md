# ğŸ—ï¸ AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise

<div align="center">

[![arXiv](https://img.shields.io/badge/arXiv-2509.10769-b31b1b.svg)](https://arxiv.org/abs/2509.10769)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

_A systematic evaluation framework for agentic AI systems across diverse architectural configurations and enterprise use cases._

</div>

---

## ğŸŒŸ Overview

AgentArch provides empirical insights into how different design dimensions interact within complex multi-agent systems. This benchmark evaluates **18 distinct agentic configurations** across state-of-the-art large language models, examining four critical system dimensions:

<table>
<tr>
<td>

### ğŸ¯ **Orchestration Strategy**

Single-agent vs. multi-agent systems

</td>
<td>

### âš™ï¸ **Agent Implementation**

ReAct vs. function calling approaches

</td>
</tr>
<tr>
<td>

### ğŸ§  **Memory Architecture**

Complete vs. summarized memory management

</td>
<td>

### ğŸ”§ **Thinking Tool Integration**

Mathematical reasoning and information synthesis tools

</td>
</tr>
</table>

---

## ğŸ” Key Findings

> **TL;DR**: No one-size-fits-all solution exists for enterprise agentic systems

| Finding                           | Impact                                                                                                | ğŸ“Š  |
| --------------------------------- | ----------------------------------------------------------------------------------------------------- | --- |
| **No Universal Architecture**     | Models demonstrate significant architectural preferences that vary by use case complexity             | ğŸ¯  |
| **Performance Gaps**              | Even top models achieve only 35.3% success on complex enterprise tasks and 70.8% on simpler workflows | ğŸ“‰  |
| **Multi-Agent ReAct Limitations** | Consistent underperformance across all models in multi-agent ReAct configurations                     | âš ï¸  |
| **Reliability Challenges**        | Pass^K scores peak at only 6.34%, indicating fundamental gaps for production deployment               | ğŸš¨  |

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/ServiceNow/AgentArch.git
cd AgentArch

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# ğŸ”‘ Replace placeholders with real API keys and endpoints
```

### Run Your First Evaluation

```python
python agent_arch/run.py \
  --mode single_agent \
  --usecase requesting_time_off \
  --model claude_sonnet_4 \
  --agent_type function_calling \
  --project test \
  --debug
```

### ğŸ³ Docker Evaluation

To run all evaluation configurations using Docker:

```bash
# Build the Docker image
docker build -f start.dockerfile -t agent-benchmark .

# Run the full benchmark suite
docker run --env-file .env \
  -e MODEL_TO_RUN=claude-sonnet-4 \
  -v $(pwd)/results:/app/results \
  agent-benchmark
```

**Environment Variables:**
- `MODEL_TO_RUN` - Required. The model to evaluate
- `PROJECT` - Project name for results (default: `default`)
- `BATCH_SIZE` - Batch size for evaluation (default: `70`)
- `DEBUG` - Enable debug mode (default: `false`)
- `K` - Pass@K trials count
- `SKIP_CONFIGS` - Comma-separated config numbers to skip

---

## ğŸ“ Repository Structure

```
AgentArch/
â”œâ”€â”€ ğŸ“ agent_arch/                    # Main module
â”‚   â”œâ”€â”€ ğŸ“ configs/
â”‚   â”‚   â”œâ”€â”€ ğŸ”§ mocked_data/
â”‚   â”‚   â”‚   â”œâ”€â”€ customer_request_routing_mocked_tool_calls.json
â”‚   â”‚   â”‚   â””â”€â”€ requesting_time_off_mocked_tool_calls.json
â”‚   â”‚   â”œâ”€â”€ âš™ï¸ use_case_configs/
â”‚   â”‚   â”‚   â”œâ”€â”€ customer_request_routing.yaml
â”‚   â”‚   â”‚   â””â”€â”€ requesting_time_off.yaml
â”‚   â”‚   â””â”€â”€ ğŸ“œ prompts.yaml
â”‚   â”œâ”€â”€ ğŸ“ tools/
â”‚   â”‚   â”œâ”€â”€ base_agent_tools.json
â”‚   â”‚   â”œâ”€â”€ base_agent_tools.py
â”‚   â”‚   â”œâ”€â”€ thinking_tools.json
â”‚   â”‚   â””â”€â”€ tool_registry.py
â”‚   â”œâ”€â”€ ğŸ“ utils/
â”‚   â”‚   â”œâ”€â”€ constants.py
â”‚   â”‚   â”œâ”€â”€ json_utils.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ model_response.py
â”‚   â”‚   â”œâ”€â”€ pass_k_metrics.py
â”‚   â”‚   â”œâ”€â”€ perf_stats.py
â”‚   â”‚   â”œâ”€â”€ run_context.py
â”‚   â”‚   â””â”€â”€ util.py
â”‚   â”œâ”€â”€ ğŸ¤– agent.py
â”‚   â”œâ”€â”€ ğŸ“Š metrics.py
â”‚   â””â”€â”€ â–¶ï¸ run.py                     # Main execution script
â”œâ”€â”€ ğŸ“ results/                        # Evaluation outputs
â”œâ”€â”€ ğŸ³ start.dockerfile                # Docker image definition
â”œâ”€â”€ ğŸ³ entrypoint.sh                   # Docker entrypoint script
â”œâ”€â”€ ğŸ“„ .env.example
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ LICENSE
â””â”€â”€ ğŸ“„ requirements.txt
```

---

## ğŸ¢ Enterprise Use Cases

<div align="center">

### 1. ğŸ“… Requesting Time Off (TO) - Simple Workflow

</div>

| Aspect            | Details                                                          |
| ----------------- | ---------------------------------------------------------------- |
| **ğŸ¯ Complexity** | Basic multi-step reasoning with clear success criteria           |
| **ğŸ› ï¸ Tools**      | 8 custom enterprise tools                                        |
| **ğŸ¤– Agents**     | 3 specialized agents                                             |
| **ğŸ’¡ Challenges** | Date calculations, leave balance verification, policy compliance |

<div align="center">

### 2. ğŸ« Customer Request Routing (CR) - Complex Workflow

</div>

| Aspect            | Details                                                         |
| ----------------- | --------------------------------------------------------------- |
| **ğŸ¯ Complexity** | Intelligent classification and escalation decisions             |
| **ğŸ› ï¸ Tools**      | 31 custom enterprise tools                                      |
| **ğŸ¤– Agents**     | 9 specialized agents                                            |
| **ğŸ’¡ Challenges** | Ambiguous request handling, context preservation, routing logic |

---

## ğŸ¤– Evaluated Models

<div align="center">

| Provider      | Models                                 | Status |
| ------------- | -------------------------------------- | ------ |
| **OpenAI**    | GPT-4.1, GPT-4o, GPT-4.1-mini, o3-mini | âœ…     |
| **Meta**      | LLaMA 3.3 70B                          | âœ…     |
| **Anthropic** | Claude Sonnet 4                        | âœ…     |

\*Framework includes support for evaluating any LiteLLM supported configuration

</div>

---

## ğŸ—ï¸ Architectural Dimensions

### ğŸ­ Orchestration Strategies

#### 1. ğŸª Orchestrator-led, Isolated Agents

_Centralized task assignment with mediated communication_

<img width="400" alt="indirect" src="https://github.com/user-attachments/assets/64310502-3a7c-4bf6-b28c-d7330db36e70" />

#### 2. ğŸŒ Orchestrator-led, Open Network

_Initial task assignment with direct agent-to-agent communication_

<img width="400" alt="direct" src="https://github.com/user-attachments/assets/e1a2a174-4761-4925-b43f-b3a1de76a929" />

#### 3. ğŸ¤– Single Agent

_Unified agent with access to all tools_

### ğŸ¨ Agent Styles

<table>
<tr>
<td align="center">

### ğŸ“ **Function Calling**

Direct tool selection using native model capabilities

</td>
<td align="center">

### ğŸ§  **ReAct**

Structured reasoning-action framework with explicit thought processes

</td>
</tr>
</table>

### ğŸ’¾ Memory Management

<table>
<tr>
<td align="center">

### ğŸ“š **Complete Memory**

Full visibility into all previous tool calls and responses

</td>
<td align="center">

### ğŸ“ **Summarized Memory**

Condensed information sharing to manage context length

</td>
</tr>
</table>

### ğŸ§® Thinking Tools

<table>
<tr>
<td align="center">

### â• **Math Tool**

Structured mathematical reasoning and calculations

</td>
<td align="center">

### ğŸ” **Synthesis Tool**

Information organization and analysis capabilities

</td>
</tr>
</table>

---

## ğŸ“Š Evaluation Metrics

### ğŸ¯ Primary Metric: Acceptable Score

Success requires **simultaneous** achievement of:

- âœ… Correct tool selection
- âœ… Accurate tool arguments (100% accuracy required)
- âœ… Correct final decision

### ğŸ”„ Reliability Metrics

- **Pass@1**: Success rate over k=8 trials
- **Pass^K**: Probability of all k trials succeeding

### ğŸ“ˆ Behavioral Metrics

- ğŸš« Hallucination rates (non-existent tool/agent selection)
- ğŸ”„ Tool repetition rates
- âŒ Missing required tools

---

## ğŸ’¡ Key Recommendations

<div align="center">

### ğŸ‘¨â€ğŸ’¼ For Practitioners

</div>

| Recommendation                                 | Rationale                                                                                |
| ---------------------------------------------- | ---------------------------------------------------------------------------------------- |
| âŒ **Avoid Multi-Agent ReAct**                 | Poor performance across all tested models                                                |
| âœ… **Use Multi-Agent for Final Decisions**     | Higher accuracy in decision-making despite tool selection challenges                     |
| ğŸ¯ **Model-Specific Architectures**            | Test multiple configurations rather than assuming universal optima                       |
| ğŸ§® **Thinking Tools for Non-Reasoning Models** | Significant performance improvements on calculation-heavy tasks for non-reasoning models |

<div align="center">

### ğŸ”¬ For Researchers

</div>

| Focus Area                               | Insight                                                                             |
| ---------------------------------------- | ----------------------------------------------------------------------------------- |
| ğŸ”„ **Architecture-Use Case Interaction** | Models perform optimally under different architectures depending on task complexity |
| âš–ï¸ **Reliability vs Performance**        | Consider both accuracy and consistency for enterprise deployment                    |
| ğŸ’¾ **Memory Management Impact**          | Minimal performance differences between complete and summarized memory              |

---

## ğŸ“š Citation

```bibtex
@misc{bogavelli2025agentarchcomprehensivebenchmarkevaluate,
      title={AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise},
      author={Tara Bogavelli and Roshnee Sharma and Hari Subramani},
      year={2025},
      eprint={2509.10769},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2509.10769},
}
```

---

## ğŸ“„ License

AgentArch is licensed under the **Apache 2.0 License**.

## ğŸ“ Contact

For questions or collaboration opportunities:

<div align="center">

[![Email](https://img.shields.io/badge/Email-tara.bogavelli%40servicenow.com-red?style=for-the-badge&logo=gmail)](mailto:tara.bogavelli@servicenow.com)

</div>

---

<div align="center">

**â­ If this project helps your research, please consider giving it a star! â­**

</div>
