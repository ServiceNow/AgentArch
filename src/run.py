import asyncio
import json
import time
import csv
import os
from datetime import datetime
import argparse
cur_dir = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(cur_dir)
if not os.getenv("DIRECTORY"):
    os.environ["DIRECTORY"] = BASE_DIR
os.environ["GOOGLE_APPLICATION_CREDENTIALS"]=os.path.join(os.environ["DIRECTORY"], "google_credentials.json")


from src.utils.util import (load_prompt_messages,
                        load_yaml_file,
                        get_agents_as_tools,
                        add_tool_call_requests_to_messages,
                        parse_react_response,
                        assign_tool_call_ids,
                        find_most_recent_matching_run,
                        load_existing_results,
                        copy_perf_stats_files,
                        get_records_to_rerun,
                        collect_existing_perf_stats,
                        add_existing_perf_stats_to_current_session,
                        normalize_result_data_types)

from src.utils.perf_stats import PerfStats
from src.agent import AGENT_REGISTRY, AgentRegistry, BaseAgent
from src.metrics import run_metrics, compute_overall_scores
from src.utils.models import model_call, get_model_info
from src.utils.run_context import RunContext

run_context = RunContext()

async def process_record(record, usecase_config: dict, model_config: dict, mode: str, thinking_tools_enabled: bool, agent_type: str, memory_type: str):
    """Runs the flow for a single record."""

    record_id = record["id"]
    usecase_directions = usecase_config["use_case_instructions"]
    run_context.add_to_memory(record_number=record_id, content={"role": "user", "content": record["user_utterance"]}, memory_type=memory_type)
    
    user_utterance = record["user_utterance"]

    messages = []
    if mode != "single_agent":
        messages = load_prompt_messages("orchestrator_initial", {
                "user_utterance": user_utterance,
                "usecase_directions": usecase_directions,
                "memory": run_context.get_memory(record_number=record_id)
            })


    max_iterations = usecase_config["orchestrator_max_iterations"]
    iterations = 0
    continue_loop = True

    while iterations < max_iterations and continue_loop:

        if "single_agent" in mode:
            passed_mode = "_".join(mode.split("_")[2:]) if mode.count("_") >= 2 else ""
            single_agent = BaseAgent(
                agent_name="single_agent",
                usecase_config=usecase_config,
                mode=passed_mode,
                model_config=model_config,
                thinking_tools_enabled=thinking_tools_enabled,
                agent_type=agent_type,
                memory_type=memory_type
            )
            finish_message = await single_agent.act(agent_objective=user_utterance, internal_record_id=record_id)
            continue_loop = False

        else:
            agents = get_agents_as_tools(usecase_config)
            agent_registry = AgentRegistry(usecase_config)
            agent_registry.register_all_from_config_file()
            model_response = await model_call(
                record_id=record_id,
                model_config=model_config,
                model_name_with_call_id=f"{model_config['name']}_orchestrator_{iterations}",
                messages=messages if agent_type == "function_calling" else load_prompt_messages("orchestrator_react", {"request": user_utterance, "usecase_directions": usecase_directions, "memory": run_context.get_memory(record_number=record_id), "agents": json.dumps(get_agents_as_tools(usecase_config), indent=4)}),
                tools=agents if agent_type == "function_calling" else None,
                tool_choice="auto" if agent_type == "function_calling" else None
            )

            response_message = model_response.llm_response
            if agent_type == "function_calling":
                agent_selections = model_response.tool_calls
            else:
                agent_selections = parse_react_response(response_message)

            if not agent_selections:
                run_context.add_to_memory(record_number=record_id, content={"role": "orchestrator", "content": response_message}, memory_type=memory_type)
                run_context.add_message_to_trace(
                    record_number=record_id,
                    agent_name="orchestrator",
                    content=response_message,
                )

                messages.append(response_message)
                continue_loop = False
            else:
                agent_selections = assign_tool_call_ids(agent_selections)
                orchestrator_content = [
                    {"agent": agent["function"]["name"], "arguments": json.loads(agent["function"]["arguments"]) if agent_type == "function_calling" else agent["function"]["arguments"]}
                    for agent in agent_selections
                ]
                run_context.add_message_to_trace(
                    record_number=record_id,
                    agent_name="orchestrator",
                    content=orchestrator_content,
                )
                run_context.add_to_memory(record_number=record_id, content={"role": "orchestrator", "content": orchestrator_content}, memory_type=memory_type)

                tool_result_messages = []
                for agent in agent_selections:
                    agent_name = agent["function"]["name"]
                    agent_args = json.loads(agent["function"]["arguments"]) if agent_type == "function_calling" else agent["function"]["arguments"]

                    agent_function = AGENT_REGISTRY.get(agent_name)
                    if agent_function is None:
                        continue

                    agent_args.update({"internal_record_id": record_id, "mode": mode, "model_config": model_config, "thinking_tools_enabled": thinking_tools_enabled, "agent_type": agent_type, "memory_type": memory_type})
                    try:
                        finish_message = await agent_function(**agent_args)
                    except Exception as e:
                        finish_message = f"Exception occurred: {e}"

                    tool_result_messages.append( {
                        "role": "tool",
                        "tool_call_id": agent["id"],
                        "name": agent_name,
                        "content": finish_message
                    })
                messages = add_tool_call_requests_to_messages(model_config["model_family"], model_response, messages, agent_selections)
                messages.extend(tool_result_messages)
        iterations += 1

    trace = run_context.get_record_trace(record_id)
    scores = run_metrics(usecase_config, trace, record["ground_truth"], mode, thinking_tools_enabled)

    return {
        "id": record_id,
        "user_utterance": user_utterance,
        "trace": trace,
        "final_memory": json.dumps(run_context.get_memory(record_id), ensure_ascii=False),
        **scores
    }

def get_records(records, k):
    records = records[:2]
    if k == 1:
        return records
    augmented_records = []
    for record in records:
        for i in range(k):
            new_id = record["id"]+f"_attempt_{i}"
            added_record = record.copy()
            added_record["id"] = new_id
            augmented_records.append(added_record)
    return augmented_records

async def run_in_batches(records, usecase_config, model_config, mode, thinking_tools_enabled, agent_type, memory_type):
    batch_size = int(os.getenv("BATCH_SIZE", 70))
    print(f"Running in batches of {batch_size}")
    results = []
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        tasks = [
            process_record(record, usecase_config, model_config, mode, thinking_tools_enabled, agent_type, memory_type)
            for record in batch
        ]
        batch_results = await asyncio.gather(*tasks)
        results.extend(batch_results)
    return results


async def main(usecase: str, model: str, mode: str, debug: bool, thinking_tools_enabled: bool, pass_k: int, project: str, agent_type: str, memory_type: str, directory: str):
    if not directory:
        directory = os.getenv("DIRECTORY")
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    results_dir = os.path.join(directory, "results", project, usecase, model, mode, "thinking_enabled" if thinking_tools_enabled else "no_thinking", agent_type, memory_type, timestamp)
    os.makedirs(results_dir, exist_ok=True)

    model_config = get_model_info(model)
    usecase_config = load_yaml_file(f"{directory}/configs/use_case_configs/{usecase}.yaml")

    RERUN_METRICS = os.getenv("RERUN_METRICS", False)
    RERUN_ERROR_RECORDS = os.getenv("RERUN_ERROR_RECORDS", False)
    results= []
    if RERUN_METRICS or RERUN_ERROR_RECORDS:
        print(f"üîÑ RERUN enabled - looking for existing results to re-run metrics on")
        results_dir_pattern = os.path.join(directory, "results", project, usecase, model, mode,"thinking_enabled" if thinking_tools_enabled else "no_thinking",agent_type, memory_type)
        previous_run_dir = find_most_recent_matching_run(results_dir_pattern, timestamp)

        if not previous_run_dir:
            print("‚ùå No previous run found with matching configuration! Try running regularly")
            print(f"   Looked in: {results_dir_pattern}")
            return
        print(f"‚úÖ Found previous run: {previous_run_dir}")
        if RERUN_ERROR_RECORDS:
            try:
                # ids_of_records_to_rerun is set of ids that need to be rerun
                model_config = json.loads(os.getenv(f"{model.upper()}_CONFIG"))
                ids_of_records_to_rerun = get_records_to_rerun(previous_run_dir, model_config["name"])
                print(f"üìä {len(ids_of_records_to_rerun)} records from previous run need to be rerun")

                existing_results = load_existing_results(previous_run_dir)
                print(f"üìã Loaded {len(existing_results)} existing records from previous run")

                existing_results_by_id = {result['id']: result for result in existing_results}
                records_to_rerun = []
                for record in usecase_config["utterances_and_ground_truths"]:
                    # Handle both pass_k format (id_attempt_X) and regular format
                    if pass_k > 1:
                        # For pass_k > 1, check if any attempt of this record needs rerunning
                        for attempt in range(pass_k):
                            record_id_with_attempt = f"{record['id']}_attempt_{attempt}"
                            if record_id_with_attempt in ids_of_records_to_rerun:
                                # Create a copy of the record with the attempt-specific ID
                                record_copy = record.copy()
                                record_copy['id'] = record_id_with_attempt # dont think this is needed, check
                                records_to_rerun.append(record_copy)
                    else:
                        # For pass_k = 1, direct ID matching
                        if record['id'] in ids_of_records_to_rerun:
                            records_to_rerun.append(record)

                print(f"üîÑ Re-running {len(records_to_rerun)} specific records...")
                new_results = await run_in_batches(records_to_rerun, usecase_config, model_config, mode, thinking_tools_enabled, agent_type, memory_type)

                final_results = []
                new_results_by_id = {result['id']: result for result in new_results}
                for existing_result in existing_results:
                    record_id = existing_result['id']
                    if record_id in new_results_by_id:
                        # Use the new result
                        final_results.append(new_results_by_id[record_id])
                        print(f"‚úÖ Replaced result for record: {record_id}")
                    else:
                        # Keep the existing result unchanged
                        final_results.append(existing_result)
                final_results = normalize_result_data_types(final_results)
                results = final_results
                print(f"‚úÖ Final merged results contain {len(results)} records")
                existing_perf_data = collect_existing_perf_stats(previous_run_dir, ids_of_records_to_rerun)
                add_existing_perf_stats_to_current_session(existing_perf_data)
                print(f"üìä Merged perf stats: kept existing data for non-rerun records, new data for rerun records")

            except Exception as e:
                print(f"‚ùå Error loading records to rerun: {e}")
                return

        elif RERUN_METRICS:
            try:
                results = load_existing_results(previous_run_dir)
                print(f"üìä Loaded {len(results)} records from previous run")

                # Re-run metrics on each result
                for i, result in enumerate(results):
                    trace = result['trace']
                    record_id = result['id']
                    user_utterance = result['user_utterance']

                    # Find the corresponding ground truth
                    ground_truth = None
                    for record in usecase_config["utterances_and_ground_truths"]:
                        # Handle both original IDs and pass_k augmented IDs
                        base_id = record_id.split("_attempt_")[0] if "_attempt_" in record_id else record_id
                        if record["id"] == base_id or record["id"] == record_id:
                            ground_truth = record["ground_truth"]
                            break

                    if not ground_truth:
                        print(f"‚ö†Ô∏è  Warning: No ground truth found for record {record_id}")
                        continue

                    scores = run_metrics(usecase_config, trace, ground_truth, mode, thinking_tools_enabled)
                    for key, value in scores.items():
                        result[key] = value

                    print(f"‚úÖ Re-scored record {i+1}/{len(results)}: {record_id}")

                # copy perf stats over
                os.makedirs(results_dir, exist_ok=True)
                copied_files = copy_perf_stats_files(previous_run_dir, results_dir)
                if copied_files:
                    print(f"üìã Copied {len(copied_files)} perf stats file(s) from previous run")


            except Exception as e:
                print(f"‚ùå Error loading previous results: {e}")
                return
    else:
        if debug:
            results = [await process_record(usecase_config["utterances_and_ground_truths"][49], usecase_config, model_config, mode, thinking_tools_enabled, agent_type, memory_type)]
        else:
            records = get_records(usecase_config["utterances_and_ground_truths"], pass_k)
            results = await run_in_batches(records, usecase_config, model_config, mode, thinking_tools_enabled, agent_type, memory_type)

    overall_scores = compute_overall_scores(results, mode, thinking_tools_enabled, pass_k)
    perf_stats = PerfStats()
    perf_file = os.path.join(results_dir, f"perf_stats_overall")
    perf_stats.generate_summary(perf_file)
    # load failure rates from perf stats file and add to overall scores
    json_path = f"{perf_file}.json"
    overall_perf_stats_dict = json.load(open(json_path, "r"))
    failure_rates = []
    for model_key in overall_perf_stats_dict:
        avg_failure_rate = overall_perf_stats_dict[model_key].get("is_failure", {}).get("average", 0)
        if avg_failure_rate:
            failure_rates.append(avg_failure_rate)
    if failure_rates:
        overall_scores.update({"failure_rate": sum(failure_rates) / len(failure_rates) if len(failure_rates) > 0 else 0})
    csv_file = os.path.join(results_dir, "record_level.csv")
    with open(csv_file, 'w', newline='') as file:
        fieldnames = results[0].keys()
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    json_file = os.path.join(results_dir, "overall_scores.json")
    with open(json_file, 'w') as f:
        json.dump(overall_scores, f, indent=4)

    duration = time.time() - time.mktime(datetime.strptime(timestamp, "%Y-%m-%d_%H-%M-%S").timetuple())
    print(f"\n‚úÖ Finished processing {len(results)} records in {duration:.2f} seconds.")
    print("\nüìà Overall Scores...")
    print(json.dumps(overall_scores, indent=4, ensure_ascii=False))
    metadata_file = os.path.join(results_dir, "metadata.json")
    with open(metadata_file, 'w') as f:
        metadata = {
            "usecase": usecase,
            "model": model,
            "mode": mode,
            "thinking_tools_enabled": thinking_tools_enabled,
            "pass_k": pass_k,
            "time_taken": f"{duration:.2f}"
        }
        if RERUN_METRICS:
            metadata["rerun_metrics"] = True
        json.dump(metadata, f, indent=4)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model",
        required=True,
        help="Choose model to run"
    )
    parser.add_argument(
        "--usecase",
        required=True,
        help="Which usecase to run"
    )
    parser.add_argument(
        "--mode",
        choices=["direct", "indirect", "single_agent", "single_agent_RAG_tool_curation", "single_agent_LLM_tool_curation"],
        required=True,
        default="direct",
        help="Choose mode: direct communication, indirect communication or single agent mode"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode"
    )
    parser.add_argument(
        "--thinking-tools-enabled",
        action="store_true",
        help="Enable thinking tools"
    )
    parser.add_argument(
        "--pass_k",
        default=1,
        type=int,
        help="Choose pass k value to compute pass@k and pass^k"
    )
    parser.add_argument(
        "--project",
        required=False,
        default="output",
        help="Project name"
    )
    parser.add_argument(
        "--agent_type",
        choices=["function_calling", "ReAct"],
        required=False,
        default="function_calling",
        help="Either function_calling or ReAct"
    )
    parser.add_argument(
        "--memory_management",
        choices=["transparent", "compact"],
        required=False,
        default="transparent",
        help="Either transparent or compact"
    )
    parser.add_argument(
        "--directory",
        required=False,
        default="",
        help="Parent directory where results directory will be created"
    )
    args = parser.parse_args()
    if args.memory_management == "compact" and args.agent_type == "ReAct":
        raise ValueError("ReAct only supports transparent memory management")
    asyncio.run(main(args.usecase,args.model, args.mode, args.debug, args.thinking_tools_enabled, args.pass_k, args.project, args.agent_type, args.memory_management, args.directory))
